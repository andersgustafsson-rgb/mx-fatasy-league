"""
Download WSX rider images directly to static/riders/wsx/
Run this script and paste the HTML/JSON data or manually provide rider URLs.
"""
import os
import re
import requests
from pathlib import Path
from urllib.parse import urljoin, urlparse

BASE_URL = "https://worldsupercrosschampionship.com"
DEST_DIR = Path("static/riders/wsx")
DEST_DIR.mkdir(parents=True, exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"
}

def slugify(s):
    if not s:
        return "unknown"
    return re.sub(r"[^a-z0-9_]", "", s.lower().replace(" ", "_").replace(".", "")).strip("_")

def download_image(url, dest_path):
    """Download an image and verify it's a valid image file."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=30, stream=True)
        resp.raise_for_status()
        
        # Check content type
        content_type = resp.headers.get('content-type', '').lower()
        if not content_type.startswith('image/'):
            print(f"   ‚ö†Ô∏è {dest_path.name} √§r inte en bild (type: {content_type})")
            return False
        
        # Save file
        dest_path.write_bytes(resp.content)
        
        # Verify it's actually an image by checking file signature
        with dest_path.open('rb') as f:
            header = f.read(4)
            if header.startswith(b'\xff\xd8\xff'):  # JPEG
                return True
            elif header.startswith(b'\x89PNG'):  # PNG
                return True
            else:
                print(f"   ‚ö†Ô∏è {dest_path.name} ser inte ut som en bildfil")
                dest_path.unlink()
                return False
    except Exception as e:
        print(f"   ‚ùå Fel vid nedladdning: {e}")
        if dest_path.exists():
            dest_path.unlink()
        return False

def extract_riders_from_page():
    """Extract rider data from the WSX riders page."""
    from bs4 import BeautifulSoup
    
    print(f"üì° H√§mtar WSX riders-sidan...")
    url = f"{BASE_URL}/riders/"
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
    except Exception as e:
        print(f"‚ùå Kunde inte h√§mta sidan: {e}")
        return []
    
    riders = []
    print(f"üì∏ S√∂ker efter rider-bilder...")
    
    # Hitta alla bilder
    images = soup.find_all('img')
    print(f"   Hittade {len(images)} bilder totalt")
    
    for img in images:
        img_src = img.get('src') or img.get('data-src') or ''
        if not img_src:
            continue
        
        # Hoppa √∂ver placeholders, ikoner, logotyper
        if any(skip in img_src.lower() for skip in ['placeholder', 'logo', 'icon', 'svg', 'banner', 'hero']):
            continue
        
        # G√∂r URL absolut
        if img_src.startswith('//'):
            img_src = 'https:' + img_src
        elif img_src.startswith('/'):
            img_src = urljoin(BASE_URL, img_src)
        
        # Hitta namn och nummer i n√§rheten av bilden
        parent = img.parent
        name = None
        number = None
        
        # G√• upp√•t i DOM-tr√§det
        for _ in range(10):
            if not parent:
                break
            
            text = parent.get_text()
            
            # Hitta nummer
            if not number:
                num_match = re.search(r'\b([1-9]\d{0,2})\b', text)
                if num_match:
                    num_val = int(num_match.group(1))
                    if 1 <= num_val <= 999:  # Rimligt f√∂rarnummer
                        number = num_val
            
            # Hitta namn (efter nummer)
            if number and not name:
                parts = text.split(str(number))
                if len(parts) > 1:
                    # Text efter nummer, f√∂re komma eller radbrytning
                    candidate = parts[1].split(',')[0].split('\n')[0].strip()
                    # Filtrera bort on√∂diga ord
                    if (candidate and len(candidate) >= 2 and len(candidate) <= 40 and
                        re.match(r'^[a-zA-Z\s]+$', candidate) and
                        candidate.upper() not in ['RIDERS', 'CHAMPIONSHIP', 'ROUND', 'GP']):
                        name = candidate
                        break
            
            # Alternativ: leta efter h1-h4 eller namn-element
            if not name:
                name_el = parent.find(['h1', 'h2', 'h3', 'h4', 'h5'], class_=re.compile(r'name|title', re.I))
                if name_el:
                    name_text = name_el.get_text().split(',')[0].split('\n')[0].strip()
                    # Filtrera bort rubriker
                    if (name_text and len(name_text) >= 2 and len(name_text) <= 40 and
                        not re.match(r'^(CHAMPIONSHIP|RIDERS|ROUND|GP|ON SALE|RESULTS|AFTER)', name_text, re.I)):
                        name = name_text
                        break
            
            parent = parent.parent if hasattr(parent, 'parent') else None
        
        # Rensa namn
        if name:
            name = re.sub(r'^(CHAMPIONSHIP|RIDERS|ROUND|GP|ON SALE|RESULTS|AFTER)\s+', '', name, flags=re.I)
            name = re.sub(r'\s+(CHAMPIONSHIP|RIDERS|ROUND|GP|ON SALE|RESULTS|AFTER)$', '', name, flags=re.I)
            name = name.strip()
        
        # Om vi har namn eller nummer, spara
        if name or number:
            riders.append({
                'name': name,
                'number': number,
                'img_url': img_src
            })
    
    # Deduplicera
    seen = set()
    unique = []
    for r in riders:
        if r['img_url'] not in seen:
            seen.add(r['img_url'])
            unique.append(r)
    
    print(f"‚úÖ Hittade {len(unique)} unika riders")
    return unique

def main():
    print("üöÄ WSX Image Downloader")
    print("=" * 50)
    
    # Prova att extrahera fr√•n sidan
    riders = extract_riders_from_page()
    
    if not riders:
        print("\n‚ùå Kunde inte hitta riders automatiskt.")
        print("\nüí° Alternativ: L√§gg in rider-data manuellt")
        print("   Redigera scriptet och l√§gg till rider-dictionary manuellt")
        return
    
    # Visa vad som hittades
    print(f"\nüìã Hittade {len(riders)} riders:")
    for i, r in enumerate(riders[:10], 1):  # Visa f√∂rsta 10
        print(f"   {i}. #{r['number'] or '?'} {r['name'] or 'Ok√§nt'}")
    if len(riders) > 10:
        print(f"   ... och {len(riders) - 10} till")
    
    # Fr√•ga om nedladdning
    print(f"\nüì• Laddar ner bilder till: {DEST_DIR.absolute()}")
    
    success = 0
    failed = 0
    
    for i, rider in enumerate(riders, 1):
        name = rider.get('name') or f"rider_{i}"
        number = rider.get('number')
        img_url = rider['img_url']
        
        # Skapa filnamn
        filename = f"{number}_{slugify(name)}.jpg" if number else f"{slugify(name)}.jpg"
        dest_path = DEST_DIR / filename
        
        # Hoppa √∂ver om filen redan finns
        if dest_path.exists():
            print(f"   [‚è≠Ô∏è] {filename} finns redan, hoppar √∂ver")
            continue
        
        print(f"   [{i}/{len(riders)}] {filename}...", end=" ")
        
        if download_image(img_url, dest_path):
            print("‚úÖ")
            success += 1
        else:
            print("‚ùå")
            failed += 1
    
    print(f"\n‚úÖ Klart!")
    print(f"   ‚úì Lyckades: {success}")
    print(f"   ‚úó Misslyckades: {failed}")
    print(f"\nüìÅ Bilder ligger i: {DEST_DIR.absolute()}")

if __name__ == "__main__":
    try:
        import bs4
    except ImportError:
        print("‚ùå BeautifulSoup4 saknas. Installera med: pip install beautifulsoup4")
        exit(1)
    
    main()

